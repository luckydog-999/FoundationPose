# client_ar_tracking.py
# ğŸš€ ç»ˆæç‰ˆï¼šåˆ©ç”¨ ZED é‡Œç¨‹è®¡å®ç° 60FPS æµç•… AR æ•ˆæœ

import pyzed.sl as sl
import cv2
import numpy as np
import socket
import struct
import lz4.frame
import time
import threading
import queue

# å¼•ç”¨æ‚¨çš„å·¥å…·å‡½æ•°
from my_utils.socket_utils import recv_json, send_json
from my_utils.math_utils import (
    matrix_to_six_dof, 
    get_tool_in_base_pose_manual,
    create_transform_matrix,
    draw_axis 
)

# --- é…ç½® ---
SERVER_IP = "127.0.0.1"
SERVER_PORT = 6006
JPEG_QUALITY = 90  # ä¿æŒé«˜ç”»è´¨

# --- å…¨å±€çŠ¶æ€ ---
# å­˜å‚¨æœ€æ–°çš„ç‰©ä½“åœ¨"ä¸–ç•Œåæ ‡ç³»"ä¸‹çš„ä½å§¿ (4x4 Matrix)
g_obj_world_pose = None 
g_last_update_time = 0
g_running = True

# è¯·æ±‚é˜Ÿåˆ— (ç”¨äºçº¿ç¨‹é€šä¿¡)
g_request_queue = queue.Queue(maxsize=1) 

def network_worker(K_list, shape):
    """ç½‘ç»œçº¿ç¨‹ï¼šè´Ÿè´£æ…¢é€Ÿçš„ä¸Šä¼ å’Œæ¥æ”¶ï¼Œä¸å¡ä¸»ç•Œé¢"""
    global g_obj_world_pose, g_last_update_time, g_running
    
    print("ğŸ”µ Network thread started...")
    
    client_sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    client_sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)
    
    try:
        client_sock.connect((SERVER_IP, SERVER_PORT))
        send_json(client_sock, {"K": K_list, "shape": shape})
        if recv_json(client_sock).get("status") != "ok": 
            print("âŒ Server Handshake failed")
            return
    except Exception as e:
        print(f"âŒ Connect Error: {e}")
        return

    while g_running:
        try:
            # 1. ä»é˜Ÿåˆ—å–æ•°æ® (é˜»å¡ç­‰å¾…)
            # data: (rgb_encoded, depth_encoded, t_cam_world_at_capture, target_name)
            data = g_request_queue.get(timeout=1.0) 
            
            rgb_enc, depth_enc, cam_pose_at_capture, target = data
            t_bytes = target.encode('utf-8')
            
            # 2. å‘é€
            head = struct.pack('>III', len(rgb_enc), len(depth_enc), len(t_bytes))
            client_sock.sendall(head + t_bytes + rgb_enc + depth_enc)
            
            # 3. æ¥æ”¶ç»“æœ (è€—æ—¶æ“ä½œ)
            res = recv_json(client_sock)
            
            if res and res.get("found"):
                # T_obj_cam: ç‰©ä½“ç›¸å¯¹äºé‚£ä¸€å¸§ç›¸æœºçš„ä½å§¿
                T_obj_cam = np.array(res['pose'])
                
                # ğŸŒŸ æ ¸å¿ƒé­”æ³•ï¼šç®—å‡ºç‰©ä½“åœ¨ä¸–ç•Œåæ ‡ç³»çš„ç»å¯¹ä½ç½®
                # T_obj_world = T_cam_world * T_obj_cam
                T_obj_world = np.dot(cam_pose_at_capture, T_obj_cam)
                
                # æ›´æ–°å…¨å±€å˜é‡
                g_obj_world_pose = T_obj_world
                g_last_update_time = time.time()
            
        except queue.Empty:
            continue
        except Exception as e:
            print(f"Network Error: {e}")
            break
            
    client_sock.close()

def main():
    global g_running, g_obj_world_pose

    # 1. åˆå§‹åŒ– ZED
    zed = sl.Camera()
    init_params = sl.InitParameters()
    init_params.camera_resolution = sl.RESOLUTION.HD720
    init_params.coordinate_units = sl.UNIT.METER
    init_params.depth_mode = sl.DEPTH_MODE.PERFORMANCE 
    if zed.open(init_params) != sl.ERROR_CODE.SUCCESS: exit()

    # ğŸŒŸ å…³é”®ï¼šå¼€å¯ä½ç½®è¿½è¸ª (Odometry)
    # è¿™å…è®¸ ZED çŸ¥é“ç›¸æœºç§»åŠ¨äº†å¤šå°‘
    track_params = sl.PositionalTrackingParameters()
    if zed.enable_positional_tracking(track_params) != sl.ERROR_CODE.SUCCESS:
        print("âŒ Positional Tracking failed to start!")
        exit()

    cam_info = zed.get_camera_information()
    calib = cam_info.camera_configuration.calibration_parameters.left_cam
    K = np.array([[calib.fx, 0, calib.cx], [0, calib.fy, calib.cy], [0, 0, 1]])
    h = cam_info.camera_configuration.resolution.height
    w = cam_info.camera_configuration.resolution.width

    # 2. å¯åŠ¨ç½‘ç»œçº¿ç¨‹
    t_net = threading.Thread(target=network_worker, args=(K.tolist(), (h, w)))
    t_net.daemon = True
    t_net.start()
    
    # 3. å‡†å¤‡æ‰‹çœ¼çŸ©é˜µ
    T_cam_tool = create_transform_matrix(
        -0.06308799, -0.12889982, -0.00412758, 358.2564, 359.5684, 358.3827
    )

    image_mat = sl.Mat()
    depth_mat = sl.Mat()
    runtime = sl.RuntimeParameters()
    runtime.enable_depth = True
    
    # ZED Pose å¯¹è±¡
    zed_pose = sl.Pose()
    
    target = "passive"
    print("ğŸš€ AR Client Running... (Press 'q' to quit)")

    # FPS ç»Ÿè®¡
    local_frames = 0
    start_time = time.time()

    while g_running:
        if zed.grab(runtime) == sl.ERROR_CODE.SUCCESS:
            local_frames += 1
            
            # --- A. è·å–å½“å‰å¸§å’Œå½“å‰ä½å§¿ ---
            zed.retrieve_image(image_mat, sl.VIEW.LEFT)
            zed.retrieve_measure(depth_mat, sl.MEASURE.DEPTH)
            
            # è·å–å½“å‰æ—¶åˆ»ç›¸æœºçš„ä¸–ç•Œä½å§¿ (T_cam_current)
            state = zed.get_position(zed_pose, sl.REFERENCE_FRAME.WORLD)
            # æ³¨æ„ï¼šTransform è½¬ numpy matrix
            T_cam_world_current = zed_pose.pose_data(sl.Transform()).m 
            
            # å‡†å¤‡å›¾åƒ
            img = image_mat.get_data()[:, :, :3]
            img = np.ascontiguousarray(img)

            # --- B. å°è¯•æŠŠæ•°æ®å¡ç»™ç½‘ç»œçº¿ç¨‹ (å¦‚æœå®ƒç©ºé—²) ---
            if g_request_queue.empty():
                depth = depth_mat.get_data()
                depth[np.isnan(depth)] = 0.0
                
                # å‹ç¼© (Uint16)
                depth_mm = (depth * 1000).astype(np.uint16)
                d_lz4 = lz4.frame.compress(depth_mm.tobytes())
                _, rgb_j = cv2.imencode('.jpg', img, [int(cv2.IMWRITE_JPEG_QUALITY), JPEG_QUALITY])
                
                # ğŸŒŸ å…³é”®ï¼šæŠŠè¿™å¼ å›¾å¯¹åº”çš„â€œæ‹æ‘„æ—¶åˆ»ç›¸æœºä½å§¿â€ä¹Ÿä¼ è¿‡å»
                # å¿…é¡»ç”¨ copy() å¦åˆ™å¯èƒ½ä¼šå˜
                pose_snapshot = T_cam_world_current.copy()
                
                g_request_queue.put((rgb_j.tobytes(), d_lz4, pose_snapshot, target))

            # --- C. æ¸²æŸ“ (AR è¡¥å¿) ---
            # å³ä½¿æœåŠ¡å™¨æ²¡è¿”å›ï¼Œæˆ‘ä»¬ä¹Ÿèƒ½ç”¨ g_obj_world_pose å’Œå½“å‰çš„ T_cam_world_current ç®—å‡ºç‰©ä½“åº”è¯¥åœ¨å“ª
            if g_obj_world_pose is not None:
                # T_obj_cam_new = inv(T_cam_world_new) * T_obj_world
                T_world_cam_current = np.linalg.inv(T_cam_world_current)
                T_obj_cam_render = np.dot(T_world_cam_current, g_obj_world_pose)
                
                # 1. ç”»è½´ (çœ‹èµ·æ¥åƒå¸åœ¨æ¡Œå­ä¸Šä¸åŠ¨)
                draw_axis(img, T_obj_cam_render, K)
                
                # 2. è®¡ç®—åŸºåº§åæ ‡ (æ˜¾ç¤ºç»™æœºæ¢°è‡‚ç”¨)
                T_tool_base = get_tool_in_base_pose_manual()
                # T_obj_base ä¸ä¼šå˜ï¼Œå› ä¸ºå®ƒæ˜¯ä¸–ç•Œåæ ‡
                # ä½†ä¸ºäº†æ˜¾ç¤ºç¨³å®šï¼Œæˆ‘ä»¬ç›´æ¥ç”¨è®¡ç®—å¥½çš„
                T_obj_base = T_tool_base @ T_cam_tool @ T_obj_cam_render
                
                x, y, z, rx, ry, rz = matrix_to_six_dof(T_obj_base)
                
                # æç¤ºä¿¡æ¯
                time_diff = time.time() - g_last_update_time
                status_color = (0, 255, 0) if time_diff < 1.0 else (0, 255, 255) # è¶…è¿‡1ç§’æ²¡æ›´æ–°å˜é»„
                
                cv2.putText(img, f"Base: {x:.3f} {y:.3f} {z:.3f}", (20, 50), 
                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, status_color, 2)
                cv2.putText(img, f"Delay: {time_diff:.1f}s", (20, 90), 
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, status_color, 1)

            # --- D. æ§åˆ¶ä¸æ˜¾ç¤º ---
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'): g_running = False
            elif key == ord('1'): target = "passive"
            elif key == ord('2'): target = "insert"

            # æ˜¾ç¤ºæœ¬åœ° FPS (åº”è¯¥æ˜¯ 30-60)
            if local_frames % 30 == 0:
                dt = time.time() - start_time
                local_fps = local_frames / dt
                local_frames = 0
                start_time = time.time()
                # print(f"Local FPS: {local_fps:.1f}") # å¯é€‰æ‰“å°

            cv2.imshow("AR Tracking Client", img)
            
    zed.close()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()